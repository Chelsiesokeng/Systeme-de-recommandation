{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ed21be-5d61-453c-a288-2572d2c82685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des Données dans Elasticsearch depuis un Notebook\n",
    "\n",
    "# Dans ce notebook, nous allons explorer comment charger un jeu de données dans Elasticsearch à partir d'un fichier dat.\n",
    "# Nous avons une installation cloud Elasticsearch, ce qui signifie que nous devrons utiliser les informations d'identification fournies \n",
    "# par l'utilisateur elastic pour nous connecter, et le mot de passe sera celui de l'API Elasticsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ff3e61-ee44-4c95-a655-61d792109d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Configuration de l'Environnement\n",
    "\n",
    "# Tout d'abord, nous devons configurer notre environnement en important les bibliothèques nécessaires \n",
    "# et en définissant nos informations d'authentification Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0703ee09-9682-4e5e-b802-3b63f0e160ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# Récupérer le mot de passe à partir d'une variable d'environnement\n",
    "# password = os.environ.get(\"ENV_PASSWORD\")\n",
    "#le mot de passe de l' API elastic\n",
    "password = \"Qn5HOUiNO7V3GWpgxXok1aeE\"\n",
    "\n",
    "# Définir l'URL Elasticsearch (IL S' AGIT DE L' URL  elasticsearch endpoint de notre deploiement movielens)\n",
    "elastic_url = \"https://2830fc520a954b858492459c95e36087.us-central1.gcp.cloud.es.io:443\"\n",
    "\n",
    "# Créer une connexion Elasticsearch\n",
    "client = Elasticsearch(hosts=[elastic_url], basic_auth=('elastic', password))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edba47d-e6a1-4e56-8590-ec48f707e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 2 : Chargement des Données\n",
    "\n",
    "# Maintenant que notre environnement est configuré, nous pouvons procéder au chargement des données. \n",
    "# Nous allons charger un jeu de données à partir des fichiers dat dans Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6d45fa-0f15-4fb2-b47c-f86054c8d991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données ratings ont été chargées dans Elasticsearch avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger les données ratings du fichier ratings.dat dans Elasticsearch\n",
    "def load_ratings_to_elasticsearch(filename, index_name):\n",
    "    # Lecture du fichier ratings.dat\n",
    "    with open(filename, 'r') as file:\n",
    "        # Parcourir chaque ligne du fichier\n",
    "        for line in file:\n",
    "            # Séparer les champs en utilisant '::' comme séparateur\n",
    "            fields = line.strip().split(\"::\")\n",
    "            # Créer un document Elasticsearch à partir des champs\n",
    "            doc = {\n",
    "                \"user_id\": int(fields[0]),\n",
    "                \"movie_id\": int(fields[1]),\n",
    "                \"rating\": float(fields[2]),\n",
    "                \"timestamp\": int(fields[3])\n",
    "            }\n",
    "            # Indexer le document dans Elasticsearch\n",
    "            yield {\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": doc\n",
    "            }\n",
    "\n",
    "# Nom de l'index Elasticsearch\n",
    "index_name = \"ratings\"\n",
    "\n",
    "# Chemin vers le fichier ratings.dat\n",
    "ratings_file = \"/home/chelsie/ml-1m/ratings.dat\"\n",
    "\n",
    "# Charger les données dans Elasticsearch en utilisant la fonction d'aide bulk\n",
    "helpers.bulk(client, load_ratings_to_elasticsearch(ratings_file, index_name))\n",
    "\n",
    "print(\"Les données ratings ont été chargées dans Elasticsearch avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1265bf56-52f9-448a-aed9-f9281feb922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données users ont été chargées dans Elasticsearch avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger les données users du fichier users.dat dans Elasticsearch\n",
    "def load_users_to_elasticsearch(filename, index_name):\n",
    "    # Lecture du fichier users.dat\n",
    "    with open(filename, 'r') as file:\n",
    "        # Parcourir chaque ligne du fichier\n",
    "        for line in file:\n",
    "            # Séparer les champs en utilisant '::' comme séparateur\n",
    "            fields = line.strip().split(\"::\")\n",
    "            # Créer un document Elasticsearch à partir des champs\n",
    "            doc = {\n",
    "                \"user_id\": int(fields[0]),\n",
    "                \"gender\": fields[1],\n",
    "                \"age\": int(fields[2]),\n",
    "                \"occupation\": fields[3],\n",
    "                \"zipcode\": fields[4]\n",
    "            }\n",
    "            # Indexer le document dans Elasticsearch\n",
    "            yield {\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": doc\n",
    "            }\n",
    "\n",
    "# Nom de l'index Elasticsearch\n",
    "index_name = \"users\"\n",
    "\n",
    "# Chemin vers le fichier users.dat\n",
    "users_file = \"/home/chelsie/ml-1m/users.dat\"\n",
    "\n",
    "# Charger les données dans Elasticsearch en utilisant la fonction d'aide bulk\n",
    "helpers.bulk(client, load_users_to_elasticsearch(users_file, index_name))\n",
    "\n",
    "print(\"Les données users ont été chargées dans Elasticsearch avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c758f838-ee9c-496d-b330-f99324204772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données movies ont été chargées dans Elasticsearch avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger les données movies du fichier movies.dat dans Elasticsearch\n",
    "def load_movies_to_elasticsearch(filename, index_name):\n",
    "    # Lecture du fichier movies.dat\n",
    "    with open(filename, 'r', encoding=\"latin1\") as file:\n",
    "        # Parcourir chaque ligne du fichier\n",
    "        for line in file:\n",
    "            # Séparer les champs en utilisant '::' comme séparateur\n",
    "            fields = line.strip().split(\"::\")\n",
    "            # Créer un document Elasticsearch à partir des champs\n",
    "            doc = {\n",
    "                \"movie_id\": int(fields[0]),\n",
    "                \"title\": fields[1],\n",
    "                \"genres\": fields[2].split(\"|\")\n",
    "            }\n",
    "            # Indexer le document dans Elasticsearch\n",
    "            yield {\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": doc\n",
    "            }\n",
    "\n",
    "# Nom de l'index Elasticsearch\n",
    "index_name = \"movies\"\n",
    "\n",
    "# Chemin vers le fichier movies.dat\n",
    "movies_file = \"/home/chelsie/ml-1m/movies.dat\"\n",
    "\n",
    "# Charger les données dans Elasticsearch en utilisant la fonction d'aide bulk\n",
    "helpers.bulk(client, load_movies_to_elasticsearch(movies_file, index_name))\n",
    "\n",
    "print(\"Les données movies ont été chargées dans Elasticsearch avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc31bf24-7fee-4e4e-b8a4-356ba331cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 3 : Récupération des Données depuis Elasticsearch\n",
    "\n",
    "# Maintenant que nous avons établi la connexion avec Elasticsearch et construit notre requête, \n",
    "# nous pouvons procéder à la récupération des données depuis Elasticsearch.\n",
    "\n",
    "# Nous utilisons la méthode search de notre client Elasticsearch pour exécuter la requête et récupérer les résultats.\n",
    "# Si le nombre de document à importer est supérieure à la limite par défaut de 10 000, \n",
    "# nous pouvons utiliser la méthode scann pour récupérer nos documents par lots de 10 000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933ef141-afcf-461b-9fc4-5af9b205b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Chargement des données de Elasticsearch vers PySpark DataFrame\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a9fb0d-3dbb-481f-add6-13c84fd77ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-------+-------+\n",
      "|age|gender|occupation|user_id|zipcode|\n",
      "+---+------+----------+-------+-------+\n",
      "|  1|     F|        10|      1|  48067|\n",
      "| 56|     M|        16|      2|  70072|\n",
      "| 25|     M|        15|      3|  55117|\n",
      "| 45|     M|         7|      4|  02460|\n",
      "| 25|     M|        20|      5|  55455|\n",
      "| 50|     F|         9|      6|  55117|\n",
      "| 35|     M|         1|      7|  06810|\n",
      "| 25|     M|        12|      8|  11413|\n",
      "| 25|     M|        17|      9|  61614|\n",
      "| 35|     F|         1|     10|  95370|\n",
      "| 25|     F|         1|     11|  04093|\n",
      "| 25|     M|        12|     12|  32793|\n",
      "| 45|     M|         1|     13|  93304|\n",
      "| 35|     M|         0|     14|  60126|\n",
      "| 25|     M|         7|     15|  22903|\n",
      "| 35|     F|         0|     16|  20670|\n",
      "| 50|     M|         1|     17|  95350|\n",
      "| 18|     F|         3|     18|  95825|\n",
      "|  1|     M|        10|     19|  48073|\n",
      "| 25|     M|        14|     20|  55113|\n",
      "+---+------+----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Requête Elasticsearch pour récupérer tous les documents de l'index \"users\"\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    },\n",
    "    \"size\": 6040\n",
    "}\n",
    "\n",
    "# Récupération des données Elasticsearch\n",
    "es_response = client.search(index=\"users\", body=query)  \n",
    "\n",
    "# Convertir les résultats en une liste de dictionnaires\n",
    "documents = [hit[\"_source\"] for hit in es_response[\"hits\"][\"hits\"]]\n",
    "\n",
    "# Convertir la liste de dictionnaires en DataFrame Spark\n",
    "es_users_df = spark.createDataFrame(documents)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "es_users_df.show()\n",
    "\n",
    "print(es_users_df.count()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82b2ddb-7afb-483f-8e98-40cb58ac1599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 05:11:34 WARN TaskSetManager: Stage 4 contains a task of very large size (11175 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+-------+\n",
      "|movie_id|rating| timestamp|user_id|\n",
      "+--------+------+----------+-------+\n",
      "|    2066|   3.0|1042049967|    146|\n",
      "|     804|   1.0| 977348138|    146|\n",
      "|    1193|   4.0| 979940868|    146|\n",
      "|    1267|   4.0| 977349660|    146|\n",
      "|    1269|   5.0| 977434285|    146|\n",
      "|    1196|   4.0| 977336700|    146|\n",
      "|    1197|   4.0| 977341318|    146|\n",
      "|    1198|   5.0| 979939805|    146|\n",
      "|    1199|   4.0| 979939730|    146|\n",
      "|     592|   3.0| 979940007|    146|\n",
      "|     594|   5.0| 977348818|    146|\n",
      "|     595|   5.0| 977348788|    146|\n",
      "|     596|   3.0| 977348869|    146|\n",
      "|     597|   4.0|1022004006|    146|\n",
      "|    2212|   4.0|1010690287|    146|\n",
      "|    2140|   2.0| 977348578|    146|\n",
      "|    1340|   4.0|1010690174|    146|\n",
      "|    2143|   2.0| 977348665|    146|\n",
      "|    2070|   2.0| 979939838|    146|\n",
      "|    2144|   3.0| 979940007|    146|\n",
      "+--------+------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 05:11:35 WARN TaskSetManager: Stage 5 contains a task of very large size (11175 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "\n",
    "# Requête Elasticsearch pour récupérer tous les documents de l'index \"ratings\"\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Utilisation de la fonction scan pour obtenir les résultats de la recherche par lots\n",
    "results = scan(client, query=query, index=\"ratings\", size=10000)\n",
    "\n",
    "# Créer une liste pour stocker les données des documents\n",
    "data = []\n",
    "\n",
    "# Parcourir les résultats\n",
    "for res in results:\n",
    "    data.append(res['_source'])\n",
    "\n",
    "# Créer un DataFrame Spark à partir de la liste de dictionnaires\n",
    "es_ratings_df = spark.createDataFrame(data)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "es_ratings_df.show()\n",
    "print(es_ratings_df.count()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d4c02-7e60-4202-a8fc-714bbda4d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparation des donn'ees ratings\n",
    "# Nous verrez que le champ timestamp est un timestamp UNIX en secondes. Elasticsearch prend les timestamps en millisecondes, \n",
    "# vous utiliserez donc quelques opérations DataFrame pour convertir les timestamps en millisecondes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52547cbe-cb0b-422d-954e-f7bd0f0a18f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 05:20:34 WARN TaskSetManager: Stage 8 contains a task of very large size (11175 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------+\n",
      "|user_id|movie_id|rating|    timestamp|\n",
      "+-------+--------+------+-------------+\n",
      "|    146|    2066|   3.0|1042049967000|\n",
      "|    146|     804|   1.0| 977348138000|\n",
      "|    146|    1193|   4.0| 979940868000|\n",
      "|    146|    1267|   4.0| 977349660000|\n",
      "|    146|    1269|   5.0| 977434285000|\n",
      "+-------+--------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Sélectionner les colonnes nécessaires et effectuer le casting sur la colonne \"timestamp\"\n",
    "ratings = es_ratings_df.select(\n",
    "    col(\"user_id\"),\n",
    "    col(\"movie_id\"),\n",
    "    col(\"rating\"),\n",
    "    (col(\"timestamp\").cast(\"long\") * 1000).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Afficher les premières lignes du DataFrame modifié\n",
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1733f60-5cbe-465b-8aec-272270f057e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 05:20:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+\n",
      "|              genres|movie_id|               title|\n",
      "+--------------------+--------+--------------------+\n",
      "|[Animation, Child...|       1|    Toy Story (1995)|\n",
      "|[Adventure, Child...|       2|      Jumanji (1995)|\n",
      "|   [Comedy, Romance]|       3|Grumpier Old Men ...|\n",
      "|     [Comedy, Drama]|       4|Waiting to Exhale...|\n",
      "|            [Comedy]|       5|Father of the Bri...|\n",
      "|[Action, Crime, T...|       6|         Heat (1995)|\n",
      "|   [Comedy, Romance]|       7|      Sabrina (1995)|\n",
      "|[Adventure, Child...|       8| Tom and Huck (1995)|\n",
      "|            [Action]|       9| Sudden Death (1995)|\n",
      "|[Action, Adventur...|      10|    GoldenEye (1995)|\n",
      "|[Comedy, Drama, R...|      11|American Presiden...|\n",
      "|    [Comedy, Horror]|      12|Dracula: Dead and...|\n",
      "|[Animation, Child...|      13|        Balto (1995)|\n",
      "|             [Drama]|      14|        Nixon (1995)|\n",
      "|[Action, Adventur...|      15|Cutthroat Island ...|\n",
      "|   [Drama, Thriller]|      16|       Casino (1995)|\n",
      "|    [Drama, Romance]|      17|Sense and Sensibi...|\n",
      "|          [Thriller]|      18|   Four Rooms (1995)|\n",
      "|            [Comedy]|      19|Ace Ventura: When...|\n",
      "|            [Action]|      20|  Money Train (1995)|\n",
      "+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du DataFrame : 3883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Requête Elasticsearch pour récupérer tous les documents de l'index \"movies\"\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    },\n",
    "    \"size\": 3883\n",
    "}\n",
    "# Récupération des données Elasticsearch\n",
    "es_response = client.search(index=\"movies\", body=query)  # Msize fait reference au nombre de documents de l' index movies\n",
    "\n",
    "# Convertir les résultats en une liste de dictionnaires\n",
    "documents = [hit[\"_source\"] for hit in es_response[\"hits\"][\"hits\"]]\n",
    "\n",
    "# Création d'une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Chargement des données Elasticsearch vers PySpark DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Convertir la liste de dictionnaires en DataFrame Spark\n",
    "es_movies_df = spark.createDataFrame(documents)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "es_movies_df.show()\n",
    "\n",
    "# Calculer la taille du DataFrame\n",
    "print(\"Taille du DataFrame :\", es_movies_df.count())\n",
    "\n",
    "# N'oubliez pas de fermer la session Spark lorsque vous avez terminé\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33822b21-0916-4b2b-bfae-0af66e94f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vous remarquerez peut-être aussi que les titres des films contiennent l'année de sortie. \n",
    "# Il serait utile de disposer de ce champ dans votre index de recherche pour filtrer les résultats \n",
    "# (par exemple, si vous souhaitez filtrer nos recommandations pour n'inclure que les films les plus récents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e1a4e16-12c3-46db-9d51-015586eae994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données des films nettoyées :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------+---------------------------+------------+\n",
      "|genres                          |movie_id|title                      |release_date|\n",
      "+--------------------------------+--------+---------------------------+------------+\n",
      "|[Animation, Children's, Comedy] |1       |Toy Story                  |1970        |\n",
      "|[Adventure, Children's, Fantasy]|2       |Jumanji                    |1970        |\n",
      "|[Comedy, Romance]               |3       |Grumpier Old Men           |1970        |\n",
      "|[Comedy, Drama]                 |4       |Waiting to Exhale          |1970        |\n",
      "|[Comedy]                        |5       |Father of the Bride Part II|1970        |\n",
      "+--------------------------------+--------+---------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "# Définir une UDF pour extraire l'année de sortie du titre des films\n",
    "def extract_year_fn(title):\n",
    "    result = re.search(\"\\(\\d{4}\\)\", title)\n",
    "    try:\n",
    "        if result:\n",
    "            group = result.group()\n",
    "            year = group[1:-1]\n",
    "            start_pos = result.start()\n",
    "            title = title[:start_pos-1]\n",
    "            return (title, year)\n",
    "        else:\n",
    "            return (title, \"1970\")\n",
    "    except:\n",
    "        print(title)\n",
    "\n",
    "# Enregistrer la fonction UDF\n",
    "extract_year = udf(extract_year_fn, StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"release_date\", StringType(), True)\n",
    "]))\n",
    "\n",
    "# Appliquer la fonction UDF à votre DataFrame des films\n",
    "es_movies_with_year = es_movies_df.withColumn(\"title\", extract_year(\"title\").title)\\\n",
    "    .withColumn(\"release_date\", extract_year(\"title\").release_date)\n",
    "\n",
    "# Afficher les données des films nettoyées\n",
    "print(\"Données des films nettoyées :\")\n",
    "es_movies_with_year.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809d892-1548-4440-914c-cc72fe5a4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer des index Elasticsearch, avec des mappings pour les utilisateurs, les films et les événements de notation\n",
    "\n",
    "# Dans Elasticsearch, un \"index\" est à peu près similaire à une \"base de données\" ou à une \"table de base de données\". \n",
    "# Le schéma d'un index s'appelle un mappage d'index.\n",
    "\n",
    "# Bien qu'Elasticsearch prenne en charge le mappage dynamique, il est conseillé de spécifier explicitement le mappage \n",
    "# lors de la création d'un index si vous savez à quoi ressemblent vos données.\n",
    "\n",
    "# Pour les besoins de votre moteur de recommandation, cela est également nécessaire pour que vous puissiez spécifier \n",
    "# le champ vectoriel qui contiendra le \"modèle\" de recommandation (c'est-à-dire les vecteurs de facteurs). \n",
    "# Lors de la création d'un champ vectoriel, vous devez fournir explicitement la dimension du vecteur, \n",
    "# de sorte qu'il ne peut s'agir d'un mappage dynamique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd47f8-6cd6-4b2a-956b-b446c16d5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chargement des DataFrames Ratings et Movies dans Elasticsearch\n",
    "\n",
    "# Tout d'abord, vous allez écrire les données d'évaluation dans Elasticsearch. \n",
    "# Notez que vous pouvez simplement utiliser le connecteur Spark Elasticsearch pour écrire un DataFrame avec \n",
    "# l'API native Spark datasource en spécifiant format(\"es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2687678f-ddde-42a4-8a2a-73d7b2b85afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created indices:\n",
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ratingsdf'}\n",
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'usersdf'}\n",
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'moviesdf'}\n"
     ]
    }
   ],
   "source": [
    "# set the factor vector dimension for the recommendation model\n",
    "VECTOR_DIM = 20\n",
    "\n",
    "create_ratings = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"timestamp\": {\n",
    "                \"type\": \"date\"\n",
    "            },\n",
    "            \"userId\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"movieId\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"rating\": {\n",
    "                \"type\": \"double\"\n",
    "            }\n",
    "        }  \n",
    "    }\n",
    "}\n",
    "\n",
    "create_users = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"userId\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"model_factor\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\" : VECTOR_DIM\n",
    "            },\n",
    "            \"model_version\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"model_timestamp\": {\n",
    "                \"type\": \"date\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "create_movies = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"movieId\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"tmdbId\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"genres\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"release_date\": {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"year\"\n",
    "            },\n",
    "            \"model_factor\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\" : VECTOR_DIM\n",
    "            },\n",
    "            \"model_version\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"model_timestamp\": {\n",
    "                \"type\": \"date\"\n",
    "            }          \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# create indices with the settings and mappings above\n",
    "res_ratings = client.indices.create(index=\"ratingsdf\", body=create_ratings)\n",
    "res_users = client.indices.create(index=\"usersdf\", body=create_users)\n",
    "res_movies = client.indices.create(index=\"moviesdf\", body=create_movies)\n",
    "\n",
    "print(\"Created indices:\")\n",
    "print(res_ratings)\n",
    "print(res_users)\n",
    "print(res_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be786d-cec7-4a89-ab71-63fbc005d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant que nous avons indexé les données de notation dans Elasticsearch, \n",
    "# nous pouvons exploiter toutes les fonctionnalités d'un moteur de recherche pour interroger les données. \n",
    "# Par exemple, nous pouvons facilement compter le nombre d'événements de notation dans une plage de dates donnée \n",
    "# en utilisant les fonctionnalités de manipulation des dates d'Elasticsearch dans une simple requête de recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa54679a-837d-4df9-8fa8-5e02200e1969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'count': 0, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count(index=\"ratingsdf\", q=\"timestamp:[2018-01-01 TO 2018-02-01]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdb0ae73-d71c-479f-ab2d-4dcd145f9648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 06:08:33 WARN TaskSetManager: Stage 18 contains a task of very large size (11175 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données de notation ont été insérées avec succès dans Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import helpers\n",
    "\n",
    "# Convertir les données de notation en un format compatible avec la méthode bulk\n",
    "rating_documents =[\n",
    "    {\n",
    "        \"_index\": \"ratingsdf\",  # Index Elasticsearch\n",
    "        \"_source\": {           # Source des données\n",
    "            \"userId\": row.user_id,\n",
    "            \"movieId\": row.movie_id,\n",
    "            \"rating\": row.rating,\n",
    "            \"timestamp\": row.timestamp\n",
    "        }\n",
    "    }\n",
    "    for row in ratings.collect()  # Parcourir les lignes du DataFrame\n",
    "]\n",
    "\n",
    "# Utiliser la méthode bulk pour insérer les documents dans Elasticsearch\n",
    "success, _ = helpers.bulk(client, rating_documents, index=\"ratingsdf\")\n",
    "\n",
    "# Vérifier si l'opération s'est bien déroulée\n",
    "if success:\n",
    "    print(\"Les données de notation ont été insérées avec succès dans Elasticsearch.\")\n",
    "else:\n",
    "    print(\"Une erreur s'est produite lors de l'insertion des données de notation dans Elasticsearch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6741ef55-c227-41f1-8178-f38ab82e3588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données des films avec années ont été insérées avec succès dans Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import helpers\n",
    "\n",
    "# Convertir les données des films avec années en un format compatible avec la méthode bulk\n",
    "movie_documents = [\n",
    "    {\n",
    "        \"_index\": \"moviesdf\",    # Index Elasticsearch\n",
    "        \"_source\": {           # Source des données\n",
    "            \"movieId\": row.movie_id,\n",
    "            \"title\": row.title,\n",
    "            \"genres\": row.genres,\n",
    "            \"release_date\": row.release_date\n",
    "        }\n",
    "    }\n",
    "    for row in es_movies_with_year.collect()  # Parcourir les lignes du DataFrame\n",
    "]\n",
    "\n",
    "# Utiliser la méthode bulk pour insérer les documents dans Elasticsearch\n",
    "success, _ = helpers.bulk(client, movie_documents, index=\"moviesdf\")\n",
    "\n",
    "# Vérifier si l'opération s'est bien déroulée\n",
    "if success:\n",
    "    print(\"Les données des films avec années ont été insérées avec succès dans Elasticsearch.\")\n",
    "else:\n",
    "    print(\"Une erreur s'est produite lors de l'insertion des données des films avec années dans Elasticsearch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1873e1e3-bb2d-4c66-bb5a-82096bd3c9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 19, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': 9.14208, 'hits': [{'_index': 'moviesdf', '_id': 'Cg-Z0I4BZ7VmenlAeLeS', '_score': 9.14208, '_source': {'movieId': 2571, 'title': 'Matrix, The', 'genres': ['Action', 'Sci-Fi', 'Thriller'], 'release_date': '1970'}}]}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test things out by searching for movies containing \"matrix\" in the title\n",
    "client.search(index=\"moviesdf\", q=\"title:matrix\", size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1ecfc-15c0-42b0-81dd-83e444243aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETAPE 4: Choix du type de modèle \n",
    "\n",
    "#     En fonction de notre cas d'utilisation, nous devons adopte le type de modèle de recommandation le plus approprié. \n",
    "# Pour commencer, Nous pouvez opter pour :\n",
    "#         Filtrage collaboratif basé sur les utilisateurs (User-Based Collaborative Filtering) : \n",
    "# recommande des éléments à un utilisateur basé sur les préférences des utilisateurs similaires.\n",
    "#         Filtrage collaboratif basé sur les articles (Item-Based Collaborative Filtering) : \n",
    "# recommande des éléments similaires à ceux que l'utilisateur a aimés dans le passé.\n",
    "#         Décomposition en valeurs singulières (Singular Value Decomposition - SVD) : \n",
    "# décompose la matrice des évaluations utilisateur-item en matrices de caractéristiques pour capturer les relations latentes.\n",
    "#         Factorisation de matrices non négatives (Non-Negative Matrix Factorization - NMF) : \n",
    "# similaire à SVD mais contraint les matrices de caractéristiques à être non négatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cbf10a-7346-42d5-a84d-f59839f13ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comme piste intéressante nous pouvons trouver des groupes d'utilisateurs ayant le même ressenti (films, genres, notes, tags).\n",
    "# Un utilisateur d'un de ces groupes sera plus disposé à apprécier les films plébiscités  par les autres membres du groupe. \n",
    "# il s' agit donc du Filtrage collaboratif basé sur les utilisateurs, nous implementerons donc ce type de filtrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f9696a0-1e77-49c9-b29d-4f8f96c500b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 07:11:22 WARN TaskSetManager: Stage 121 contains a task of very large size (12675 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 121:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------+\n",
      "|movieId|rating|   timestamp|userId|\n",
      "+-------+------+------------+------+\n",
      "|   1625|   5.0|975421312000|   793|\n",
      "|    953|   5.0|975422430000|   793|\n",
      "|   2431|   4.0|975421125000|   793|\n",
      "|   3095|   4.0|975421987000|   793|\n",
      "|    969|   4.0|975422943000|   793|\n",
      "+-------+------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 07:11:26 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 121 (TID 274): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "\n",
    "# Requête Elasticsearch pour récupérer tous les documents de l'index \"ratings\"\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Utilisation de la fonction scan pour obtenir les résultats de la recherche par lots\n",
    "results = scan(client, query=query, index=\"ratingsdf\", size=10000)\n",
    "\n",
    "# Créer une liste pour stocker les données des documents\n",
    "data = []\n",
    "\n",
    "# Parcourir les résultats\n",
    "for res in results:\n",
    "    data.append(res['_source'])\n",
    "\n",
    "# Créer un DataFrame Spark à partir de la liste de dictionnaires\n",
    "ratings_from_es = spark.createDataFrame(data)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "ratings_from_es.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8839aaa7-06f6-4da6-8b5a-86b1ee1ed780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 07:14:37 WARN TaskSetManager: Stage 122 contains a task of very large size (12675 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 122:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+------+\n",
      "|movieId|rating|          timestamp|userId|\n",
      "+-------+------+-------------------+------+\n",
      "|   1625|   5.0|2000-11-28 15:21:52|   793|\n",
      "|    953|   5.0|2000-11-28 15:40:30|   793|\n",
      "|   2431|   4.0|2000-11-28 15:18:45|   793|\n",
      "|   3095|   4.0|2000-11-28 15:33:07|   793|\n",
      "|    969|   4.0|2000-11-28 15:49:03|   793|\n",
      "+-------+------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 07:14:41 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 122 (TID 275): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "# Convertir le timestamp en format de date lisible\n",
    "ratings_from_es = ratings_from_es.withColumn(\"timestamp\", from_unixtime(ratings_from_es.timestamp / 1000).alias(\"timestamp\"))\n",
    "\n",
    "# Afficher les premières lignes du DataFrame avec le timestamp converti\n",
    "ratings_from_es.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08a623db-c8bd-444c-9257-20079f7ba3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 07:15:48 WARN TaskSetManager: Stage 123 contains a task of very large size (12675 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/12 07:15:53 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 123 (TID 276): Attempting to kill Python Worker\n",
      "24/04/12 07:15:53 WARN TaskSetManager: Stage 124 contains a task of very large size (12675 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facteurs d'utilisateurs:\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "| 10|[0.70144784, 0.41...|\n",
      "| 20|[0.91114306, 0.0,...|\n",
      "| 30|[0.3812649, 0.0, ...|\n",
      "| 40|[0.5583849, 0.100...|\n",
      "| 50|[0.61492586, 0.0,...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Facteurs d'éléments (films):\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "| 10|[0.8227385, 0.0, ...|\n",
      "| 20|[0.6605314, 0.227...|\n",
      "| 30|[0.0, 0.0, 0.7150...|\n",
      "| 40|[0.04367039, 0.0,...|\n",
      "| 50|[0.42771438, 0.52...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Définir le modèle ALS\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", regParam=0.02, rank=VECTOR_DIM, seed=54,coldStartStrategy=\"drop\", nonnegative=True)\n",
    "\n",
    "# Entraîner le modèle ALS sur les données chargées depuis Elasticsearch\n",
    "model = als.fit(ratings_from_es)\n",
    "\n",
    "# Afficher les premières lignes des facteurs d'utilisateurs\n",
    "print(\"Facteurs d'utilisateurs:\")\n",
    "model.userFactors.show(5)\n",
    "\n",
    "# Afficher les premières lignes des facteurs d'éléments (films)\n",
    "print(\"Facteurs d'éléments (films):\")\n",
    "model.itemFactors.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ce0a456-aed0-4865-a649-565886a13356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+---------------+\n",
      "| id|        model_factor|   model_version|model_timestamp|\n",
      "+---+--------------------+----------------+---------------+\n",
      "| 10|[0.8227385, 0.0, ...|ALS_6867d8ccd66c|     1712899519|\n",
      "| 20|[0.6605314, 0.227...|ALS_6867d8ccd66c|     1712899519|\n",
      "| 30|[0.0, 0.0, 0.7150...|ALS_6867d8ccd66c|     1712899519|\n",
      "| 40|[0.04367039, 0.0,...|ALS_6867d8ccd66c|     1712899519|\n",
      "| 50|[0.42771438, 0.52...|ALS_6867d8ccd66c|     1712899519|\n",
      "+---+--------------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+----------------+---------------+\n",
      "| id|        model_factor|   model_version|model_timestamp|\n",
      "+---+--------------------+----------------+---------------+\n",
      "| 10|[0.70144784, 0.41...|ALS_6867d8ccd66c|     1712899519|\n",
      "| 20|[0.91114306, 0.0,...|ALS_6867d8ccd66c|     1712899519|\n",
      "| 30|[0.3812649, 0.0, ...|ALS_6867d8ccd66c|     1712899519|\n",
      "| 40|[0.5583849, 0.100...|ALS_6867d8ccd66c|     1712899519|\n",
      "| 50|[0.61492586, 0.0,...|ALS_6867d8ccd66c|     1712899519|\n",
      "+---+--------------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp, unix_timestamp\n",
    "\n",
    "# Récupération de l'identifiant unique du modèle ALS\n",
    "ver = model.uid\n",
    "\n",
    "# Sélection des colonnes nécessaires à partir du DataFrame des facteurs d'éléments (films)\n",
    "# - Sélection de la colonne \"id\" pour l'identifiant de l'élément\n",
    "# - Sélection de la colonne \"features\" et renommage en \"model_factor\" pour le vecteur de facteurs du modèle\n",
    "# - Ajout d'une colonne \"model_version\" avec la valeur de l'identifiant unique du modèle\n",
    "# - Ajout d'une colonne \"model_timestamp\" avec la valeur du timestamp actuel en format Unix\n",
    "ts = unix_timestamp(current_timestamp())\n",
    "movie_vectors = model.itemFactors.select(\"id\",\\\n",
    "                                         col(\"features\").alias(\"model_factor\"),\\\n",
    "                                         lit(ver).alias(\"model_version\"),\\\n",
    "                                         ts.alias(\"model_timestamp\"))\n",
    "movie_vectors.show(5)\n",
    "\n",
    "# Sélection des colonnes nécessaires à partir du DataFrame des facteurs d'utilisateurs\n",
    "# - Sélection de la colonne \"id\" pour l'identifiant de l'utilisateur\n",
    "# - Sélection de la colonne \"features\" et renommage en \"model_factor\" pour le vecteur de facteurs du modèle\n",
    "# - Ajout d'une colonne \"model_version\" avec la valeur de l'identifiant unique du modèle\n",
    "# - Ajout d'une colonne \"model_timestamp\" avec la valeur du timestamp actuel en format Unix\n",
    "user_vectors = model.userFactors.select(\"id\",\\\n",
    "                                        col(\"features\").alias(\"model_factor\"),\\\n",
    "                                        lit(ver).alias(\"model_version\"),\\\n",
    "                                        ts.alias(\"model_timestamp\"))\n",
    "user_vectors.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e58b34d-c4f8-4458-b956-683e38597de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Créer une session Spark en spécifiant le package Elasticsearch\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"movielensrecommendationsystem\") \\\n",
    "    .config(\"spark.jars.packages\", \"/home/chelsie/ml-1m/elasticsearch-spark-20_2.11-8.13.2.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823517bb-f405-48de-a977-dceccd93c5b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'spark' from 'elasticsearch' (/home/chelsie/.local/lib/python3.10/site-packages/elasticsearch/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melasticsearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spark \u001b[38;5;28;01mas\u001b[39;00m es_spark\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## Écriture des données dans Elasticsearch pour les vecteurs de films\u001b[39;00m\n\u001b[1;32m      4\u001b[0m movie_vectors\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes.mapping.id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes.write.operation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoviesdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'spark' from 'elasticsearch' (/home/chelsie/.local/lib/python3.10/site-packages/elasticsearch/__init__.py)"
     ]
    }
   ],
   "source": [
    "from elasticsearch import spark as es_spark\n",
    "\n",
    "## Écriture des données dans Elasticsearch pour les vecteurs de films\n",
    "movie_vectors.write.format(\"es\") \\\n",
    "    .option(\"es.mapping.id\", \"id\") \\\n",
    "    .option(\"es.write.operation\", \"update\") \\\n",
    "    .save(\"moviesdf\", mode=\"append\")\n",
    "\n",
    "# Écriture des données dans Elasticsearch pour les vecteurs d'utilisateurs\n",
    "user_vectors.write.format(\"es\") \\\n",
    "    .option(\"es.mapping.id\", \"id\") \\\n",
    "    .option(\"es.write.operation\", \"index\") \\\n",
    "    .save(\"users\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bbd41b45-f9cd-4bd9-bf2a-0d781d71aca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données des facteurs de films ont été insérées avec succès dans Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "\n",
    "# Convertir les données de facteurs de films en un format compatible avec la méthode bulk\n",
    "factor_documents = [\n",
    "    {\n",
    "        \"_index\": \"movies_factor\",  # Index Elasticsearch\n",
    "        \"_id\": row.id,              # ID du document\n",
    "        \"_source\": {                # Source des données\n",
    "            \"model_factor\": row.model_factor,\n",
    "            \"model_version\": row.model_version,\n",
    "            \"model_timestamp\": row.model_timestamp\n",
    "        }\n",
    "    }\n",
    "    for row in movie_vectors.collect()  # Parcourir les lignes du DataFrame movie_vectors\n",
    "]\n",
    "\n",
    "# Utiliser la méthode bulk pour envoyer les données à Elasticsearch\n",
    "success, _ = bulk(client, factor_documents)\n",
    "\n",
    "# Vérifier si l'opération s'est bien déroulée\n",
    "if success:\n",
    "    print(\"Les données des facteurs de films ont été insérées avec succès dans Elasticsearch.\")\n",
    "else:\n",
    "    print(\"Une erreur s'est produite lors de l'insertion des données des facteurs de films dans Elasticsearch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757e44b-2494-47a3-a46f-57ebf21e9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les données de facteurs des utilisateurs en un format compatible avec la méthode bulk\n",
    "user_factor_documents = [\n",
    "    {\n",
    "        \"_index\": \"users_factor\",  # Index Elasticsearch\n",
    "        \"_id\": row.id,             # ID du document\n",
    "        \"_source\": {               # Source des données\n",
    "            \"model_factor\": row.model_factor,\n",
    "            \"model_version\": row.model_version,\n",
    "            \"model_timestamp\": row.model_timestamp\n",
    "        }\n",
    "    }\n",
    "    for row in user_vectors.collect()  # Parcourir les lignes du DataFrame user_vectors\n",
    "]\n",
    "\n",
    "# Utiliser la méthode bulk pour envoyer les données à Elasticsearch\n",
    "success, _ = bulk(client, user_factor_documents)\n",
    "\n",
    "# Vérifier si l'opération s'est bien déroulée\n",
    "if success:\n",
    "    print(\"Les données des facteurs des utilisateurs ont été insérées avec succès dans Elasticsearch.\")\n",
    "else:\n",
    "    print(\"Une erreur s'est produite lors de l'insertion des données des facteurs des utilisateurs dans Elasticsearch.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
